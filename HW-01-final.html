<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Homework-1:</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="HW-01-final_files/libs/clipboard/clipboard.min.js"></script>
<script src="HW-01-final_files/libs/quarto-html/quarto.js"></script>
<script src="HW-01-final_files/libs/quarto-html/popper.min.js"></script>
<script src="HW-01-final_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="HW-01-final_files/libs/quarto-html/anchor.min.js"></script>
<link href="HW-01-final_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="HW-01-final_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="HW-01-final_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="HW-01-final_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="HW-01-final_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Homework-1:</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Author</strong>: “J. Hickman”</p>
<p>Neural network training is essentially a multi-variable optimization of a scalar field <span class="math display">\[y=f(\mathbf x)=f(x_1,x_2 ... x_N)\]</span></p>
<ul>
<li>The local or global minima or maxima of <span class="math inline">\(y=f(\mathbf x)=f(x_1,x_2 ... x_N)\)</span> satisfy the condition <span class="math inline">\(\mathbf \nabla f(x) = 0\)</span> where <span class="math inline">\(\mathbf \nabla f(x)=(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2} \dots \frac{\partial f}{\partial x_N})\)</span> is the gradient, i.e.&nbsp;the vector of partial derivatives.</li>
<li>The most common form of numerical optimizers are “gradient based” methods.</li>
<li>Such methods “follow” the gradient, i.e.&nbsp;the vector of partial derivatives, ‘downhill’ into local minima in the optimization space.</li>
<li>Therefore in this homework we focus on some fundamental concepts associated with numerical optimization, including numerical derivatives, solvers, and optimizers.</li>
<li>We then apply these tools to a simple regression example.</li>
</ul>
<p><strong>Instructions</strong></p>
<ul>
<li>Read and work through all tutorial content and do all exercises below</li>
</ul>
<p><strong>Submission:</strong></p>
<ul>
<li>You need to upload ONE document to Canvas when you are done
<ul>
<li><ol type="1">
<li>A PDF (or HTML) of the completed form of this notebook<br>
</li>
</ol></li>
</ul></li>
<li><strong>IMPORTANT</strong>: Please render with <code>quarto render HW-1.ipynb</code>
<ul>
<li>(please add <code>embed-resource: true</code> in the yaml header)</li>
</ul></li>
<li>The final uploaded version should NOT have any code-errors present</li>
<li>All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc</li>
</ul>
<section id="part-0-reading" class="level1">
<h1>Part-0: Reading</h1>
<p>Please read the following chapter in the course textbook if you have not done so already</p>
<p><img src="images/reading.png" class="img-fluid"></p>
</section>
<section id="part-1-numerical-math" class="level1">
<h1>Part-1: Numerical math</h1>
<ul>
<li>The point of this section is to
<ul>
<li><ol type="1">
<li>Get you familiar with a few numerical methods (e.g.&nbsp;numerical differentiation)</li>
</ol></li>
<li><ol start="2" type="1">
<li><code>Sanity check</code> your functions with simple cases where we KNOW the answer beforehand.</li>
</ol></li>
</ul></li>
</ul>
<p><strong>Mathematical paradigms</strong></p>
<ul>
<li>When performing mathematics, we typically have two options, as demonstrated in the following image.</li>
<li>Often, it is preferable to take the “numerical” route. In this case we just let the computer do the work for us, rather than computing derivatives or integrals by hand with pen and paper.</li>
<li>This is the route that will be typically be used in this course, however, during module-1 you will be expected to also do some pen and paper work.</li>
</ul>
<p><img src="images/paradigms.png" class="img-fluid"></p>
<section id="numerical-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="numerical-derivatives">Numerical Derivatives</h2>
<p>Given a function <span class="math inline">\(y=f(x)\)</span>, we can compute a derivative, denoted <span class="math inline">\(f'(x)\)</span>, with respect to it’s independent variable <span class="math inline">\(x\)</span>.</p>
<p>The derivative has the meaning of the instantaneous slope of the function at any point point <span class="math inline">\(x\)</span>, i.e.&nbsp;the slope of the tangent line.</p>
<p>We can approximate the tangent line, via the secant line, using any two points on the curve.</p>
<p>The closer the points are, i.e.&nbsp;the smaller <span class="math inline">\(\Delta x\)</span>, the better the approximation.</p>
<p>This idea is summarized in the following image</p>
<p><img src="images/derivative.png" class="img-fluid"></p>
<p>When functions in the computer are define on a “mesh” of dependent variable locations.</p>
<p>E.g. if the function has time-dependence, the in-dependent variable becomes <span class="math inline">\(t\)</span> instead of <span class="math inline">\(x\)</span>.</p>
<p>Due to the finite precision (and memory) of a computer, we can only store functions as list, not continuous functions. As demonstrated in the following image</p>
<p><img src="images/image-2.png" class="img-fluid"></p>
<p>Therefore, the secant line is a typical method for computing a derivative inside computers.</p>
<p>Approximating the derivative with the secant line is also known as the finite difference approximation of the derivative.</p>
<p>Later in the course, we will discuss a more sophisticated and elegant method for computing the derivative, known as “back propagation”. The back-propagation method is more commonly used in deep learning, however, finite differences are easier to understand educationally and are also an important computational tool in many algorithms. Therefore, we will start with finite differences before moving onto back-propagation.</p>
<p>There are several different finite-difference formulas for the first derivative of varying accuracy, as shown in the following image.</p>
<p>For more see: <a href="https://en.wikipedia.org/wiki/Finite_difference">https://en.wikipedia.org/wiki/Finite_difference</a></p>
<p><strong>IMPORTANT:</strong> This method can easily be generalized to compute the gradient (vector of partial derivatives) of multi-variable scalar field.</p>
<p><img src="images/image-3.png" class="img-fluid"></p>
</section>
<section id="assignment-1" class="level2">
<h2 class="anchored" data-anchor-id="assignment-1">Assignment-1</h2>
<ul>
<li>Write a function <code>def dy_dx(x,y)</code> that takes two numpy arrays x,y where y=f(x)
<ul>
<li>Returns a vector, of same dimension as x and y, with the first derivative, computed using finite difference</li>
<li>For dy_dx[0] use forward difference</li>
<li>for dy_dx[1:N-1] use central difference</li>
<li>for dy_dx[N] us backward difference</li>
<li>Note this can be done with three lines in numpy, using a “vectorized” formalism, OR with a for-loop</li>
</ul></li>
<li>Test your function by plotting the derivative of <span class="math inline">\(f(x)=x^3+x^2\)</span> with both the numerical and analytic solution, where the derivative is <span class="math inline">\(\frac{df}{dx}=3x^2+2x\)</span></li>
<li><strong>IMPORTANT</strong>: In ANN training the we compute a different derivative <span class="math inline">\(\frac{\partial L}{\partial w}\)</span>, where <span class="math inline">\(L\)</span> is the loss surface and <span class="math inline">\(w\)</span> are the model parameters.</li>
</ul>
<div id="855ab9b9" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8beabe4b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># INSERT CODE HERE</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dy_dx(x, y):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate the derivative of y with respect to x using the finite difference.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    x (numpy array): The x values.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y (numpy array): The y values corresponding to f(x).</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    numpy array: The derivatives of y with respect to x.</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#create an array to store the derivatives</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    dy <span class="op">=</span> np.zeros_like(y)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#using methods mentioned above for dy_dx[0], dy_dx[1:N-1], dy_dx[N-1]}</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Forward the differences for the first point</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    dy[<span class="dv">0</span>] <span class="op">=</span> (y[<span class="dv">1</span>] <span class="op">-</span> y[<span class="dv">0</span>]) <span class="op">/</span> (x[<span class="dv">1</span>] <span class="op">-</span> x[<span class="dv">0</span>])</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">#get the central differences for the middle points</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    dy[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> (y[<span class="dv">2</span>:] <span class="op">-</span> y[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">/</span> (x[<span class="dv">2</span>:] <span class="op">-</span> x[:<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#backward diff for the last point</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    dy[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> (y[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> y[<span class="op">-</span><span class="dv">2</span>]) <span class="op">/</span> (x[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> x[<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dy</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">#test functions and derivatives provided above</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dx(x):</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>, <span class="dv">100</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co">#get vals for y </span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f(x)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>analytical_derivative <span class="op">=</span> df_dx(x)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co">#get vals for numerical derivative</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>numerical_derivative <span class="op">=</span> dy_dx(x, y)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.plot(x, analytical_derivative, label<span class="op">=</span><span class="st">'Analytical Derivative'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.plot(x, numerical_derivative, label<span class="op">=</span><span class="st">'Numerical Derivative'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparing Analytical and Numerical Derivatives'</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'dy/dx'</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW-01-final_files/figure-html/cell-3-output-1.png" width="808" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This method can easily be generalized to to compute the partial derivative terms of the gradient.</p>
<p>In the following image h is the step size (i.e.&nbsp;<span class="math inline">\(h=\Delta x\)</span>)</p>
<p><img src="images/partial.png" class="img-fluid"></p>
</section>
<section id="assignment-2" class="level2">
<h2 class="anchored" data-anchor-id="assignment-2">Assignment-2</h2>
<p><code>Sanity check</code></p>
<ul>
<li>The previous sub-assignment computed the derivate at each x-point, however, often we just need to compute the “local” derivative around one x point denoted x0</li>
<li>Write a python function <code>def f(x)</code> which returns a multi-variable scalar field <span class="math inline">\(y=f(\mathbf x)=f(x_1,x_2 ... x_N)\)</span>, an arbitrary input dimension N.
<ul>
<li>For simplicity, just use <span class="math inline">\(y=f(\mathbf x)=f(x_1,x_2,x_3)=2.718 x_1^2+3.14 x_2+1.0 x_3+2\)</span></li>
<li>compute the gradient of this function “by hand” to make sure your numerical implementation matches the analytical result <br><br></li>
</ul></li>
<li>Write a function <code>def grad(f,x0,dx)</code> that;
<ul>
<li>takes this f function as an input
<ul>
<li>see the following link for information on passing a function as an argument to another function.</li>
<li><a href="https://www.geeksforgeeks.org/passing-function-as-an-argument-in-python/">https://www.geeksforgeeks.org/passing-function-as-an-argument-in-python/</a></li>
</ul></li>
<li>Returns the gradient vector, as a numpy array, computed around the point x0 by using the multi-variable finite difference with a step size equal to dx.</li>
</ul></li>
<li>test the function by computing the gradient vector around <span class="math inline">\(\mathbf x=(x_1,x_2,x_3)=(1,2,3)\)</span></li>
</ul>
<div id="9ee69af2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># INSERT YOUR CODE HERE</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">2.718</span> <span class="op">*</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">3.14</span> <span class="op">*</span> x[<span class="dv">1</span>] <span class="op">+</span> <span class="fl">1.0</span> <span class="op">*</span> x[<span class="dv">2</span>] <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(f, x0, dx):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> np.zeros_like(x0)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#iterate through the elements of x0</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x0)):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        f_diff <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        b_diff <span class="op">=</span> []</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> val <span class="kw">in</span> x0:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            <span class="co">#append the value of x0 + dx to f_diff</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            f_diff.append(x0[val<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> dx)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">#append the value of x0 - dx to b_diff</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            b_diff.append(x0[val<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> dx)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> (f(f_diff) <span class="op">-</span> f(b_diff)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dx)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gradient</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])  <span class="co">#test</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="fl">.0001</span> <span class="co">#step size</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"---both methods returned the gradient as 9.576... with x0 = 1,2,3---</span><span class="ch">\n\n</span><span class="ss">"</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co">#get the numerical gradient</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>grad_n <span class="op">=</span> grad(f, x0, dx)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Numerical Gradient at x0 ="</span>, x0, <span class="st">":"</span>, grad_n)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co">#gradient taken by hand, only 2.718 has an exponent, 3.14 and 1 remain, 2 doesnt have a variable x and is 0</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>grad_a <span class="op">=</span> np.array([<span class="fl">2.718</span> <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> x0[<span class="dv">0</span>] <span class="op">+</span> <span class="fl">3.14</span> <span class="op">+</span> <span class="fl">1.0</span>])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Analytical Gradient at x0 ="</span>, x0, <span class="st">":"</span>, grad_a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>---both methods returned the gradient as 9.576... with x0 = 1,2,3---


Numerical Gradient at x0 = [1 2 3] : 9.57600000000447
Analytical Gradient at x0 = [1 2 3] : [9.576]</code></pre>
</div>
</div>
<div id="db70cf62" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradient(f, p, x, y, epsilon<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute numerical gradient using central difference."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> np.zeros_like(p)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(p)):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        p_plus <span class="op">=</span> np.copy(p)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        p_minus <span class="op">=</span> np.copy(p)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        p_plus[i] <span class="op">+=</span> epsilon</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        p_minus[i] <span class="op">-=</span> epsilon</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        grad[i] <span class="op">=</span> (f(x, p_plus, y) <span class="op">-</span> f(x, p_minus, y)) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> epsilon)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="part-3-regression" class="level1">
<h1>Part-3: Regression</h1>
<section id="assignment-3" class="level2">
<h2 class="anchored" data-anchor-id="assignment-3">Assignment-3</h2>
<p><code>Code a multi-variable optimizer</code> (feel free to recycle code if you have done this in other classes)</p>
<ul>
<li>The following sub-assignment is a copy of <code>Lab-1.1</code>, <strong>however</strong>, you will need to code the optimizer yourself, rather than using the Scipy optimizer.</li>
<li>Copy the regression workflow from your COMPLETED lab-1.1 assignment</li>
<li>You MUST use the <code>dataset="5D_LINEAR"; model_type="linear"</code> dataset (but your code should work for any of them)</li>
<li>Replace the SciPy optimizer with your own optimizer function</li>
<li>Have the optimizer take the objective (loss) function as an argument, along with various default options
<ul>
<li><code>def optimizer(objective, algo=‘GD’, LR=0.001, method=‘batch’):</code></li>
</ul></li>
<li>Code the optimizer so it can train using batch, mini-batch or stochastic paradigms
<ul>
<li>For minibatch use a 0.5 batch size</li>
</ul></li>
<li>Code the following optimizers for a general N-dimensional optimization problem (i.e don’t hardcode for a specific dimensionality)
<ul>
<li>Gradient decent (GD)</li>
<li>GD+momentum</li>
<li>RMSprop</li>
<li>Nelder-Mead (optional extra credit +2.5 points)</li>
<li>For gradient based methods compute derivatives numerically using finite difference</li>
</ul></li>
<li><code>IMPORTANT:</code> Add a dense feed forward fully connected neural network option to your <code>m(x,p)</code> function
<ul>
<li>code the ANN from scratch using numpy</li>
<li>choose the network size and activation function yourself, as appropriate for a regression problem</li>
</ul></li>
<li>Train on the dataset using both the ANN and the linear regression model</li>
<li>Visualize the results for both linear and ANN cases
<ul>
<li>Report the loss value for training and validation sets</li>
<li>A <code>parity plot</code> where you plot <code>y_pred</code> vs <code>y_data</code> for both validation and training data (Note: the line y=x represents a perfect fit)</li>
<li>A “time-series plot” showing “training_error” and “validation_error” as a function of the ith iteration of the optimizer</li>
</ul></li>
</ul>
<div id="8ab9329f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>dataset<span class="op">=</span><span class="st">"5D_LINEAR"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model_type<span class="op">=</span><span class="st">"linear"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># DATA PARAM</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>xmin<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>xmax<span class="op">=</span><span class="dv">10</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> s(x):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(dataset<span class="op">==</span><span class="st">"2D_LINEAR"</span> <span class="kw">or</span> dataset<span class="op">==</span><span class="st">"2D_LOGISTIC"</span>):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#DEFINE A RANDOM INPUT MATRIX (X) ITH TWO FEATURES</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>np.random.uniform(xmin,xmax,(N,<span class="dv">2</span>))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#DEFINE RESPONSE VARIABLES Y USING A LINEAR RESPONSE</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    noise<span class="op">=</span>np.random.uniform(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>,(N,))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="fl">1.4</span><span class="op">*</span>x[:,<span class="dv">0</span>]<span class="op">+</span><span class="fl">4.5</span><span class="op">*</span>x[:,<span class="dv">1</span>]<span class="op">+</span><span class="dv">5</span><span class="op">+</span>noise</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>y.reshape(N,<span class="dv">1</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(dataset<span class="op">==</span><span class="st">"2D_LOGISTIC"</span>):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>      <span class="co">#I'm not sure this is necessary, but I added it because it seemed logical</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>      model_type<span class="op">=</span><span class="st">"logistic"</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>      y<span class="op">=</span>s(y)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span>(dataset<span class="op">==</span><span class="st">"5D_LINEAR"</span>):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#DEFINE A RANDOM INPUT MATRIX (X) ITH TWO FEATURES</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>np.random.uniform(xmin,xmax,(N,<span class="dv">5</span>))</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#DEFINE RESPONSE VARIABLES Y USING A LINEAR RESPONSE</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    noise<span class="op">=</span>np.random.uniform(<span class="op">-</span><span class="fl">0.5</span>,<span class="fl">0.5</span>,(N,))</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="fl">1.4</span><span class="op">*</span>x[:,<span class="dv">0</span>]<span class="op">+</span><span class="fl">4.5</span><span class="op">*</span>x[:,<span class="dv">1</span>]<span class="op">+</span><span class="fl">3.5</span><span class="op">*</span>x[:,<span class="dv">2</span>]<span class="op">+</span><span class="fl">8.6</span><span class="op">*</span>x[:,<span class="dv">3</span>]<span class="op">-</span><span class="fl">1.5</span><span class="op">*</span>x[:,<span class="dv">4</span>]<span class="op">+</span><span class="dv">5</span><span class="op">+</span>noise</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>y.reshape(N,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1f866bc0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of x: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of y: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of x: (2000, 5)
Shape of y: (2000, 1)</code></pre>
</div>
</div>
<div id="047f543f" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>split_idx <span class="op">=</span> <span class="bu">int</span>(N<span class="op">*</span><span class="fl">.8</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x_train, x_test <span class="op">=</span> x[:split_idx], x[split_idx:]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> y[:split_idx], y[split_idx:]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">,</span> y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">,</span> y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training set shape: ((1600, 5), (1600, 1))
Test set shape: ((400, 5), (400, 1))</code></pre>
</div>
</div>
<div id="caf5a8a1" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>u_x <span class="op">=</span> np.mean(x_train, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>sigma_x <span class="op">=</span> np.std(x_train, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> model_type <span class="op">==</span> <span class="st">"linear"</span>:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    u_y <span class="op">=</span> np.mean(y_train)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    sigma_y <span class="op">=</span> np.std(y_train)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    u_y <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    sigma_y <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> (x_train <span class="op">-</span> u_x) <span class="op">/</span> sigma_x</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>x_val <span class="op">=</span> (x_test <span class="op">-</span> u_x) <span class="op">/</span> sigma_x</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> model_type <span class="op">==</span> <span class="st">"linear"</span>:</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> (y_train <span class="op">-</span> u_y) <span class="op">/</span> sigma_y</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    y_val <span class="op">=</span> (y_test <span class="op">-</span> u_y) <span class="op">/</span> sigma_y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="54b8b6c1" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> m(x, p, model_type<span class="op">=</span><span class="st">'linear'</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model_type <span class="op">==</span> <span class="st">'linear'</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        p<span class="op">=</span>p.reshape(p.shape[<span class="dv">0</span>],<span class="dv">1</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        z<span class="op">=</span>np.dot(x, p[<span class="dv">1</span>:])<span class="op">+</span>p[<span class="dv">0</span>]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> model_type <span class="op">==</span> <span class="st">'ANN'</span>:</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Artificial Neural Network model with a single hidden layer."""</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        W1, b1, W2, b2 <span class="op">=</span> p[<span class="st">'W1'</span>], p[<span class="st">'b1'</span>], p[<span class="st">'W2'</span>], p[<span class="st">'b2'</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        Z1 <span class="op">=</span> np.dot(x, W1) <span class="op">+</span> b1</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        A1 <span class="op">=</span> relu(Z1)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        Z2 <span class="op">=</span> np.dot(A1, W2) <span class="op">+</span> b2</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Z2</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Invalid model type. Choose 'linear' or 'ANN'."</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(Z):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""ReLU activation function."""</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, Z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="711c4ba9" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimizer(objective, params, x_train, y_train, algo<span class="op">=</span><span class="st">'GD'</span>, LR<span class="op">=</span><span class="fl">0.001</span>, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Optimizes model parameters to minimize the objective function.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - objective: The objective (loss) function to minimize.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - params: Initial model parameters.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - x_train: Training input data.</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_train: Training target data.</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - algo: Optimization algorithm ('GD', 'GD_MOMENTUM', 'RMSprop').</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - LR: Learning rate.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - epochs: Number of training epochs.</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - batch_size: Size of batches for mini-batch training. If None, use full batch.</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">    - Optimized parameters.</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> x_train.shape[<span class="dv">0</span>]</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_size <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> batch_size <span class="op">&gt;</span> n_samples:</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> n_samples  <span class="co"># Full batch if batch_size is None or larger than the number of samples</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize velocity for momentum and squared gradient for RMSprop</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> {k: np.zeros_like(v) <span class="cf">for</span> k, v <span class="kw">in</span> params.items()} <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>) <span class="cf">else</span> np.zeros_like(params)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> {k: np.zeros_like(v) <span class="cf">for</span> k, v <span class="kw">in</span> params.items()} <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>) <span class="cf">else</span> np.zeros_like(params)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mini-batch or stochastic training</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_samples, batch_size):</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            end <span class="op">=</span> start <span class="op">+</span> batch_size</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            x_batch, y_batch <span class="op">=</span> x_train[start:end], y_train[start:end]</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute gradients</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>            grads <span class="op">=</span> compute_gradients(objective, params, x_batch, y_batch)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update parameters based on the selected algorithm</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> algo <span class="op">==</span> <span class="st">'GD'</span>:</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">-=</span> LR <span class="op">*</span> grads[key]</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> algo <span class="op">==</span> <span class="st">'GD_MOMENTUM'</span>:</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>                    v[key] <span class="op">=</span> <span class="fl">0.9</span> <span class="op">*</span> v[key] <span class="op">-</span> LR <span class="op">*</span> grads[key]  <span class="co"># Update velocity</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">+=</span> v[key]  <span class="co"># Update parameters</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> algo <span class="op">==</span> <span class="st">'RMSprop'</span>:</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>                    s[key] <span class="op">=</span> <span class="fl">0.9</span> <span class="op">*</span> s[key] <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> (grads[key] <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Update squared gradient</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">-=</span> LR <span class="op">*</span> grads[key] <span class="op">/</span> (np.sqrt(s[key]) <span class="op">+</span> <span class="fl">1e-7</span>)  <span class="co"># Update parameters</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradients(f, p, dx<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> {}</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(p, <span class="bu">dict</span>):  <span class="co"># For ANN parameters</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key <span class="kw">in</span> p.keys():</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> np.zeros_like(p[key])</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> index, _ <span class="kw">in</span> np.ndenumerate(p[key]):</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>                p_plus <span class="op">=</span> {k: np.array(v, copy<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> k, v <span class="kw">in</span> p.items()}</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>                p_minus <span class="op">=</span> {k: np.array(v, copy<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> k, v <span class="kw">in</span> p.items()}</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>                p_plus[key][index] <span class="op">+=</span> dx</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>                p_minus[key][index] <span class="op">-=</span> dx</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>                loss_plus <span class="op">=</span> f(p_plus)  <span class="co"># Evaluate loss at p_plus</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>                loss_minus <span class="op">=</span> f(p_minus)  <span class="co"># Evaluate loss at p_minus</span></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>                grad[index] <span class="op">=</span> (loss_plus <span class="op">-</span> loss_minus) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dx)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>            grads[key] <span class="op">=</span> grad</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:  <span class="co"># For linear model parameters</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> np.zeros_like(p)</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(p)):</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>            p_plus <span class="op">=</span> np.array(p, copy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>            p_minus <span class="op">=</span> np.array(p, copy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>            p_plus[i] <span class="op">+=</span> dx</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>            p_minus[i] <span class="op">-=</span> dx</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>            loss_plus <span class="op">=</span> f(p_plus)  <span class="co"># Evaluate loss at p_plus</span></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>            loss_minus <span class="op">=</span> f(p_minus)  <span class="co"># Evaluate loss at p_minus</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>            grads[i] <span class="op">=</span> (loss_plus <span class="op">-</span> loss_minus) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> dx)</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2896e3a6" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(x_train, y_train, x_val, y_val, model, optimizer, params_init, epochs, learning_rate, algo<span class="op">=</span><span class="st">'GD'</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, beta<span class="op">=</span><span class="fl">0.99</span>, epsilon<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> params_init.copy()  <span class="co"># Start with initial parameters</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize optimization variables differently for dict (ANN) and array (linear)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    v, s <span class="op">=</span> (<span class="bu">dict</span>(), <span class="bu">dict</span>()) <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>) <span class="cf">else</span> (np.zeros_like(params), np.zeros_like(params))</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define a lambda function that computes the loss using 'x_train' and 'y_train'</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        objective_function <span class="op">=</span> <span class="kw">lambda</span> p: mse_loss(model(x_train, p), y_train)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gradients using the objective function</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> compute_gradients(objective_function, params, dx<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> algo <span class="op">==</span> <span class="st">'GD'</span>:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Standard Gradient Descent update</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>):  <span class="co"># For ANN</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">-=</span> learning_rate <span class="op">*</span> grads[key]</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:  <span class="co"># For linear model</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>                params <span class="op">-=</span> learning_rate <span class="op">*</span> grads</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> algo <span class="op">==</span> <span class="st">'GD_MOMENTUM'</span>:</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Gradient Descent with Momentum update</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>):  <span class="co"># For ANN</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                    v[key] <span class="op">=</span> momentum <span class="op">*</span> v[key] <span class="op">-</span> learning_rate <span class="op">*</span> grads[key]  <span class="co"># Update velocity</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">+=</span> v[key]  <span class="co"># Update parameters</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:  <span class="co"># For linear model</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>                v <span class="op">=</span> momentum <span class="op">*</span> v <span class="op">-</span> learning_rate <span class="op">*</span> grads</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>                params <span class="op">+=</span> v</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> algo <span class="op">==</span> <span class="st">'RMSprop'</span>:</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># RMSprop update</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(params, <span class="bu">dict</span>):  <span class="co"># For ANN</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> key <span class="kw">in</span> params.keys():</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                    s[key] <span class="op">=</span> beta <span class="op">*</span> s[key] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta) <span class="op">*</span> (grads[key] <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Update squared gradient</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                    params[key] <span class="op">-=</span> learning_rate <span class="op">*</span> grads[key] <span class="op">/</span> (np.sqrt(s[key]) <span class="op">+</span> epsilon)  <span class="co"># Update parameters</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:  <span class="co"># For linear model</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>                s <span class="op">=</span> beta <span class="op">*</span> s <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta) <span class="op">*</span> (grads <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>                params <span class="op">-=</span> learning_rate <span class="op">*</span> grads <span class="op">/</span> (np.sqrt(s) <span class="op">+</span> epsilon)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally, print the loss every 100 epochs or at the last epoch</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> epoch <span class="op">==</span> epochs <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">=</span> mse_loss(model(x_train, params), y_train)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">=</span> mse_loss(model(x_val, params), y_val)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Training Loss = </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">, Validation Loss = </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(y_pred, y_true):</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Mean Squared Error loss."""</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="50b75aae" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_parameters(n_features, model_type<span class="op">=</span><span class="st">'linear'</span>, hidden_layers_sizes<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Initializes parameters for the specified model type.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - n_features: The number of features in the input data.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - model_type: The type of model ('linear' or 'ANN').</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - hidden_layers_sizes: A list containing the sizes of each hidden layer for the ANN model.</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - The initialized parameters. For linear models, this is a NumPy array. </span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">      For ANNs, this is a dictionary containing weights and biases for each layer.</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model_type <span class="op">==</span> <span class="st">'linear'</span>:</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize parameters for linear regression model</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> np.random.randn(n_features <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.01</span>  <span class="co"># Small random values for weights and bias</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> model_type <span class="op">==</span> <span class="st">'ANN'</span>:</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hidden_layers_sizes <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"hidden_layers_sizes must be provided for ANN model type"</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> {}</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        layer_sizes <span class="op">=</span> [n_features] <span class="op">+</span> hidden_layers_sizes <span class="op">+</span> [<span class="dv">1</span>]  <span class="co"># Adding input and output layer sizes</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and biases for each layer in the ANN</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(layer_sizes)):</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>            params[<span class="st">'W'</span> <span class="op">+</span> <span class="bu">str</span>(l)] <span class="op">=</span> np.random.randn(layer_sizes[l<span class="op">-</span><span class="dv">1</span>], layer_sizes[l]) <span class="op">*</span> np.sqrt(<span class="fl">2.</span> <span class="op">/</span> layer_sizes[l<span class="op">-</span><span class="dv">1</span>])  <span class="co"># He initialization</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>            params[<span class="st">'b'</span> <span class="op">+</span> <span class="bu">str</span>(l)] <span class="op">=</span> np.zeros((<span class="dv">1</span>, layer_sizes[l]))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Invalid model type. Choose 'linear' or 'ANN'."</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="84160410" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_parity(y_pred, y_true, title<span class="op">=</span><span class="st">'Parity Plot'</span>):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    plt.scatter(y_true, y_pred, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    plt.plot([y_true.<span class="bu">min</span>(), y_true.<span class="bu">max</span>()], [y_true.<span class="bu">min</span>(), y_true.<span class="bu">max</span>()], <span class="st">'r--'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Actual'</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Predicted'</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(losses):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(losses, label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Training Loss Over Time'</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fd5e5e19" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training parameters</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>algo <span class="op">=</span> <span class="st">'GD'</span>  <span class="co"># Change to 'GD+momentum' or 'RMSprop' as needed</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>model_type <span class="op">=</span> <span class="st">'linear'</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>params_linear <span class="op">=</span> initialize_parameters(n_features<span class="op">=</span><span class="dv">5</span>, model_type<span class="op">=</span>model_type)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>trained_params_linear <span class="op">=</span> train_model(x_train, y_train, x_val, y_val, <span class="kw">lambda</span> x, p: m(x, p, model_type), optimizer, params_linear, epochs, learning_rate, algo<span class="op">=</span>algo)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>y_pred_train_linear <span class="op">=</span> m(x_train, trained_params_linear, model_type)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>y_pred_val_linear <span class="op">=</span> m(x_val, trained_params_linear, model_type)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>plot_parity(y_pred_val_linear, y_val, title<span class="op">=</span><span class="st">'Linear Model Parity Plot'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: Training Loss = 0.9687738974083954, Validation Loss = 0.9309599814603539
Epoch 100: Training Loss = 0.015377774088813996, Validation Loss = 0.014628631772032996
Epoch 200: Training Loss = 0.0002721199424217459, Validation Loss = 0.00024342065579544838
Epoch 300: Training Loss = 2.6445985391341408e-05, Validation Loss = 2.239989091865402e-05
Epoch 400: Training Loss = 2.2334955005649024e-05, Validation Loss = 2.0267796030500923e-05
Epoch 500: Training Loss = 2.2263963126991968e-05, Validation Loss = 2.043246805291481e-05
Epoch 600: Training Loss = 2.226269345264172e-05, Validation Loss = 2.0461568023220304e-05
Epoch 700: Training Loss = 2.2262669844851734e-05, Validation Loss = 2.0465500941146882e-05
Epoch 800: Training Loss = 2.2262669387026018e-05, Validation Loss = 2.0466015837657636e-05
Epoch 900: Training Loss = 2.2262669377750394e-05, Validation Loss = 2.0466082803649988e-05
Epoch 999: Training Loss = 2.2262669377554387e-05, Validation Loss = 2.0466091454100495e-05</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW-01-final_files/figure-html/cell-15-output-2.png" width="513" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="51ff23dc" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training parameters</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>algo <span class="op">=</span> <span class="st">'GD'</span>  <span class="co"># Change to 'GD+momentum' or 'RMSprop' as needed</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training and Testing for ANN Model</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>model_type <span class="op">=</span> <span class="st">'ANN'</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>params_ann <span class="op">=</span> initialize_parameters(n_features<span class="op">=</span><span class="dv">5</span>, model_type<span class="op">=</span>model_type, hidden_layers_sizes<span class="op">=</span>[<span class="dv">10</span>])</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>trained_params_ann <span class="op">=</span> train_model(x_train, y_train, x_val, y_val, <span class="kw">lambda</span> x, p: m(x, p, model_type), optimizer, params_ann, epochs, learning_rate, algo<span class="op">=</span>algo)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>y_pred_train_ann <span class="op">=</span> m(x_train, trained_params_ann, model_type)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>y_pred_val_ann <span class="op">=</span> m(x_val, trained_params_ann, model_type)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>plot_parity(y_pred_val_ann.flatten(), y_val.flatten(), title<span class="op">=</span><span class="st">'ANN Model Parity Plot'</span>)  <span class="co"># Flatten if necessary</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: Training Loss = 3.136515263778554, Validation Loss = 3.0147646404668365
Epoch 100: Training Loss = 0.2608823597177453, Validation Loss = 0.25855482018429404
Epoch 200: Training Loss = 0.16289813355959187, Validation Loss = 0.16501002772995027
Epoch 300: Training Loss = 0.12231880471486477, Validation Loss = 0.12487242762791456
Epoch 400: Training Loss = 0.09574512552281014, Validation Loss = 0.09837956647605313
Epoch 500: Training Loss = 0.07478420037905568, Validation Loss = 0.07791543049584096
Epoch 600: Training Loss = 0.05750942126455804, Validation Loss = 0.06136479042120874
Epoch 700: Training Loss = 0.04366309153507708, Validation Loss = 0.04802055877793309
Epoch 800: Training Loss = 0.033079713417556056, Validation Loss = 0.03739008215879078
Epoch 900: Training Loss = 0.025172705789064154, Validation Loss = 0.029235280793957975
Epoch 999: Training Loss = 0.01960432069600253, Validation Loss = 0.023132311666350224</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="HW-01-final_files/figure-html/cell-16-output-2.png" width="513" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="part-4-analytical" class="level1">
<h1>Part-4: Analytical</h1>
<section id="assignment-4" class="level2">
<h2 class="anchored" data-anchor-id="assignment-4">Assignment-4</h2>
<ul>
<li>Note: feel free to recycle solutions if you have done this in other classes</li>
<li>Work through the following math problems relevant to deep learning (show your work and all steps)</li>
<li>You can do this either by (1) doing the math inside <code>.ipynb</code> using LaTex (recommended) OR (2) doing it with pen and paper, taking photographs, and embedding the solutions into the <code>.ipynb</code> as images.</li>
</ul>
<p><img src="images/2023-09-07-14-05-07.png" class="img-fluid"></p>
</section>
<section id="assignment-5" class="level2">
<h2 class="anchored" data-anchor-id="assignment-5">Assignment-5</h2>
<ul>
<li>Linear least squares (LLS): <code>Single-variable</code></li>
<li>Use Calculus to analytically derive the expression for single variable linear regression fitting parameters using the sum of square error as the loss function (show your work).</li>
<li>That is, do the mathematical optimization problem by hand rather than with a computer <span class="math display">\[
\begin{aligned}
&amp; \text { Model: } y=M(x \mid \mathbf{p})=m x+b \,\,\,\, \mathbf{p}=\left(p_0, p_1\right)=(m, b) \\
&amp; \text { Loss surface: } L(\mathbf{p})=L(m, b)=\sum_{i=1}^N\left(\hat{y}_i-M\left(\hat{x}_i, m, b\right)\right)^2 \\
&amp;
\end{aligned}
\]</span></li>
</ul>
<p><strong>Solution</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; m=\frac{\operatorname{cov}(x, y)}{\operatorname{var}(x)} \quad \bar{x}=\frac{1}{N} \sum_{i=1}^N x_i \quad \operatorname{var}(X)=\frac{1}{N} \sum_{i=1}^n\left(x_i-\bar{x}\right)^2 \\
&amp; b=\bar{y}-\frac{\operatorname{cov}(x, y)}{\operatorname{var}(x)} \bar{x} \quad \bar{y}=\frac{1}{N} \sum_{i=1}^N y_i \quad \operatorname{cov}(X, Y)=\frac{1}{N} \sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right) \\
&amp;
\end{aligned}
\]</span></p>
</section>
<section id="assignment-6" class="level2">
<h2 class="anchored" data-anchor-id="assignment-6">Assignment-6</h2>
<ul>
<li>Linear least squares (LLS): <code>Multi-variable</code></li>
<li>Use matrix calculus to analytically derive the expression for two variable linear regression fitting parameters using the sum of square error as the loss function</li>
<li>Show your work using matrix notation</li>
<li>From your solution infer the generalized solution for an arbitrary number of variables</li>
<li>solution: <span class="math inline">\(\quad \vec{w}=\left(X^{\top} X\right)^{-1} X^{\top} Y\)</span>.</li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>