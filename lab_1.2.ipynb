{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7mv7zywpnFG"
      },
      "source": [
        "## Backpropagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETGXNWOI94mR"
      },
      "source": [
        "**Problem-1:**. In this problem, you will write code to perform backpropagation from scratch. Consider a network with 2 inputs, 1 hidden layer containing 3 units, and a single categorical output.\n",
        "\n",
        "Imagine the two input nodes are labeled $x_1$ and $x_2$, the three hidden nodes are $a_1$, $a_2$, and $a_3$, and the output node is $\\hat{y}$. The edges from the input layer to the hidden layer are labeled $w_{ij}$ for going from input $x_i$ to hidden node $a_j$, and the edges from the hidden layer to the output layer are $v_j$, from hidden unit $a_j$ to $\\hat{y}$. There is a bias vector for the hidden layer $\\mathbf{b}$ and a bias constant $c$ for the output layer.\n",
        "\n",
        "Let $a_j = \\max(0, z_j)$, where $z_j$ is a weighted sum from the previous layer plus a bias unit. That is,\n",
        "$$\n",
        "z_1 = w_{11}x_1 + w_{21}x_2 + b_1 \\\\\n",
        "z_2 = w_{12}x_1 + w_{22}x_2 + b_2 \\\\\n",
        "z_3 = w_{13}x_1 + w_{23}x_2 + b_3 \\\\\n",
        "\\Rightarrow z_j = w_{1j}x_1 + w_{2j}x_2 + b_j\n",
        "$$\n",
        "\n",
        "For the output, we write\n",
        "\n",
        "$$ \\hat{y} = g(v_1a_1 + v_2a_2 + v_3a_3 + c), $$\n",
        "where $g$ is the output function (in this case, for binary classification, $g$ is the sigmoid function). Expanding the above expression to show $\\hat{y}$ as a function of all of the variables of the graph, we obtain\n",
        "$$ \\hat{y} = g\\big[v_1\\max(0, w_{11}x_1 + w_{21}x_2 + b_1) \\\\ + v_2\\max(0, w_{12}x_1 + w_{22}x_2 + b_2) \\\\ + v_3\\max(0, w_{13}x_1 + w_{23}x_2 + b_3) + c\\large].$$\n",
        "\n",
        "We can express this succinctly using matrix notation. If\n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "w_{11} &w_{12} &w_{13}\\\\\n",
        "w_{21} &w_{22} &w_{23}\\\\\n",
        "\\end{bmatrix}, \\hspace{.5cm} \\mathbf{x} = \\begin{bmatrix} x_1 \\\\x_2 \\end{bmatrix}, \\hspace{.5cm} \\mathbf{b} = \\begin{bmatrix} b_1 \\\\b_2 \\\\b_3\\end{bmatrix}, \\hspace{.5cm} \\mathbf{a} = \\begin{bmatrix} a_1 \\\\a_2 \\\\a_3\\end{bmatrix}, \\hspace{.5cm} \\text{and} \\hspace{.5cm} \\mathbf{v} = \\begin{bmatrix} v_1 \\\\v_2 \\\\v_3\\end{bmatrix},$$\n",
        "\n",
        "then\n",
        "$$\n",
        "z_j = W^{\\text{T}}\\mathbf{x} + \\mathbf{b}, \\hspace{.5cm} a_j = \\max(0, z_j), \\hspace{.5cm} \\text{and} \\hspace{.5cm} \\hat{y} = g(\\mathbf{v}^{\\text{T}}\\cdot\\mathbf{a} + c).\n",
        "$$\n",
        "\n",
        "(A) Derive expressions of the gradient of the loss function with respect to each of the model parameters.\n",
        "\n",
        "(B) Write a function `grad_f(...)` that takes in a weights vector and returns the gradient of the loss at that location. You will also need to write a number of helper functions.\n",
        "\n",
        "(C) Generate a synthetic dataset resembling an XOR pattern. This has been done for you.\n",
        "\n",
        "(D) Fit the network with gradient descent. Keep track of the total loss at each iteration. Create a plot of the loss over training iterations.\n",
        "\n",
        "(E) Repeat (D) but using momentum. Compare and contrast the results.\n",
        "\n",
        "(F) Plot a visualization of the final decision boundary, and overlay the decision boundary over the XOR dataset you created in (C)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Below we provide some starter code to get you started`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOUR ANSWER FOR (A) HERE**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define sigmoid activation \n",
        "def sig(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#define relu activation\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "#determine the derivative of relu function, either 1 or 0\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(int)\n",
        "\n",
        "#now we can create a function to compute the gradiant of L with respect to W and b using model parameters defined in problem 1\n",
        "def get_grad(x, y, W, b, v, c):\n",
        "    #forward pass\n",
        "    z = np.dot(W.T, x) + b #get weighted sum in hidden layer\n",
        "    a = relu(z) #use relu function created earlier\n",
        "    y_hat = sig(np.dot(v.T, a) + c) #get the output of the model\n",
        "\n",
        "    #backward pass\n",
        "    dL_dyhat = -y / y_hat + (1 - y) / (1 - y_hat) #find the derivative of L with respect to y_hat\n",
        "\n",
        "    dyhat_dv = a #y_hat is linear combination of a's, so derivative is just a\n",
        "    dyhat_dc = 1 #y_hat is linear combination of 1's, so derivative is just 1\n",
        "\n",
        "    #get gradients of L for v and c\n",
        "    dL_dv = dL_dyhat * dyhat_dv #chain rule to find derivative of L with respect to v\n",
        "    dL_dc = dL_dyhat * dyhat_dc #chain rule to find derivative of L with respect to c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W is [[-0.07460603  0.09334357 -0.0479048 ]\n",
            " [ 0.0794473  -0.02465006 -0.03275565]]\n",
            "b is [[-0.00972471  0.06805102 -0.07537957]]\n",
            "v is [[ 0.00860524]\n",
            " [-0.02539755]\n",
            " [-0.01040064]]\n",
            "c is -0.07411186405649742\n"
          ]
        }
      ],
      "source": [
        "# Code for (B) here\n",
        "\n",
        "np.random.seed(123456)\n",
        "\n",
        "W_init = 0.2*np.random.rand(2,3)-0.1\n",
        "b_init = 0.2*np.random.rand(1,3)-0.1\n",
        "v_init = 0.2*np.random.rand(3,1)-0.1\n",
        "c_init = 0.2*np.random.rand()-0.1\n",
        "\n",
        "params_init = [W_init, b_init, v_init, c_init]\n",
        "\n",
        "print(f\"W is {W_init}\\nb is {b_init}\\nv is {v_init}\\nc is {c_init}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sigmoid = # Your code here\n",
        "relu = # Your code here\n",
        "\n",
        "sigmoid_prime = # Your code here\n",
        "relu_prime = # Your code here\n",
        "\n",
        "def loss(y, y_hat):\n",
        "    \"\"\"Compute loss for a single example.\"\"\"\n",
        "\n",
        "def loss_derivative(y, y_hat):\n",
        "    \"\"\"Compute derivative of loss\"\"\"\n",
        "\n",
        "def yhat(X, params):\n",
        "    \"\"\"Compute yhat given input data and model specification\"\"\"\n",
        "\n",
        "def grad_f(params, x, y):\n",
        "    \"\"\"Compute gradients\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for (C) here\n",
        "\n",
        "np.random.seed(12345)\n",
        "\n",
        "Xs = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000, 1))\n",
        "Ys = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000, 1))\n",
        "Zs = np.logical_xor(Xs<0, Ys>0).reshape((1000, 1))\n",
        "\n",
        "plt.scatter(Xs, Ys, c=Zs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code for (D) here\n",
        "def gradient_descent(x, y, starting_params, iterations, lr):\n",
        "    \"\"\"Perform gradient descent\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xs = np.random.uniform(low=-2, high=2, size=1000).reshape((500,2))\n",
        "ys = np.zeros(500)\n",
        "ys[np.logical_and(xs[:,0]>0, xs[:,1]>0)]=1\n",
        "ys[np.logical_and(xs[:,0]<0, xs[:,1]<0)]=1\n",
        "\n",
        "xs = np.asmatrix(xs)\n",
        "ys = np.asmatrix(ys).reshape((500,1))\n",
        "\n",
        "trajectories_standard, losses_standard = gradient_descent(xs, ys, params_init,\n",
        "                                        iterations=3000, lr=1e-4)\n",
        "plt.plot(losses_standard)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gradient Descent\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code for (E) here\n",
        "\n",
        "def gradient_descent_momentum(x, y, starting_params, iterations, lr, alpha):\n",
        "    \"\"\"Perform gradient descent with momentum\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trajectories_momentum, losses_momentum = gradient_descent_momentum(xs, ys, params_init,\n",
        "                                        iterations=3000, lr=1e-4, alpha=0.5)\n",
        "plt.plot(losses_momentum)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gradient Descent with Momentum\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(losses_standard, label=\"Gradient Descent\")\n",
        "plt.plot(losses_momentum, label=\"Gradient Descent with Momentum\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Vanilla Gradient Descent v. Gradient Descent with Momentum\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code for (F) here\n",
        "\n",
        "Xs = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Ys = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Zs = np.logical_xor(Xs<0, Ys>0).reshape((1000,1))\n",
        "\n",
        "XOR = np.concatenate((Xs, Ys, Zs), axis=1)\n",
        "\n",
        "x = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "y = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "Z = np.zeros(250*250).reshape(250,250)\n",
        "\n",
        "\n",
        "W_final, b_final, v_final, c_final = trajectories_momentum[-1]\n",
        "\n",
        "\n",
        "def ff_nn_relu(X, W, b, v, c):\n",
        "    \"\"\"Computes yhat given input data and model specification\"\"\"\n",
        "\n",
        "# Given the XOR dataset, compute the decision boundary learned by the network\n",
        "for countx, i in enumerate(x):\n",
        "    for county, j in enumerate(y):\n",
        "        temp = np.array([i[0],j[0]])\n",
        "        Z[countx][county] = # Your code here\n",
        "\n",
        "\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "cmap = colors.ListedColormap(['green', 'red'])\n",
        "plt.contourf(X, Y, Z, cmap=cmap)\n",
        "plt.scatter(Xs, Ys, c=Zs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYjg1USW0qP9"
      },
      "source": [
        "**Problem-2** One of the challenges in training neural models is when inputs are not on the same scales. Why is this problematic? Consider the expression for the derivative of the loss with respect to a weight for a particular layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxMDgsG3Ub7c"
      },
      "source": [
        "**YOUR ANSWER HERE**:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
