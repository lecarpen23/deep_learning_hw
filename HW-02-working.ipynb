{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "format: \n",
    "    html: \n",
    "        embed-resources: true\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: \n",
    "\n",
    "`ANN training in Keras or Pytorch & Hyper-parameter tuning`\n",
    "\n",
    "## Overview \n",
    "\n",
    "* Classification is one of the most common forms of supervised machine learning \n",
    "* In this homework we will explore \"model tuning\" for the case of a multi-class classification problem, as applied the MNIST data set\n",
    "* `You can do this assignment in either Keras OR PyTorch` (or both), it is your choice.\n",
    "\n",
    "## Submission \n",
    "\n",
    "* You need to upload TWO documents to Canvas when you are done\n",
    "  * (1) A PDF (or HTML) of the completed form of the `HW-2.ipynb` document \n",
    "* The final uploaded version should NOT have any code-errors present \n",
    "* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n",
    "\n",
    "`IMPORTANT`: THERE ARE MANY WAYS TO DO THIS, SO FEEL FREE TO DEVIATE SLIGHTLY FROM THE EXACT DETAILS, BUT THE OVERALL RESULT AND FLOW SHOULD MATCH WHAT IS OUTLINED BELOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.1: Data preparation\n",
    "\n",
    "* Normalize the data as needed\n",
    "* Partition data into training, validation, and test (i.e. leave one out CV)\n",
    "  * One option to do this is to give these arrays global scope so they are seen inside the training function (so they don't need to be passed to functions)\n",
    "* **Optional but recommended:** Create a K-fold cross validation data set, rather than just doing leave one out\n",
    "* Do any other preprocessing you feel is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Fashion MNIST\n",
    "\n",
    "99.999% sure Dr. Hickman said it was ok to use Fashion MNIST instead of regular MNIST, which I will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data below using ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424486c8c8554e559c54ff9a4f512aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77e79fc66da4ffe91ace7d7b5461d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa3f86387c848a39997dcb1bec0107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e2d2ed73f74e9cb6d00e637a0a9091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 60000\n",
      "Length Test: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length Train: {len(train_data)}')\n",
    "print(f'Length Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randperm() received an invalid combination of arguments - got (float, generator=torch._C.Generator), but expected one of:\n * (int n, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data) \u001b[38;5;241m-\u001b[39m train_size\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#make val 10% of train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataset.py:349\u001b[0m, in \u001b[0;36mrandom_split\u001b[1;34m(dataset, lengths, generator)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):    \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 349\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Subset(dataset, indices[offset \u001b[38;5;241m-\u001b[39m length : offset]) \u001b[38;5;28;01mfor\u001b[39;00m offset, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(_accumulate(lengths), lengths)]\n",
      "\u001b[1;31mTypeError\u001b[0m: randperm() received an invalid combination of arguments - got (float, generator=torch._C.Generator), but expected one of:\n * (int n, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = .9 * len(train_data)\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "#make val 10% of train\n",
    "train_data, val_data = random_split(train_data, lengths=[train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 54000\n",
      "Length Val: 6000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length Train: {len(train_data)}')\n",
    "print(f'Length Val: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little heavy on the test data, so I went with just 10% of the traing data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2da1719ae00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2da1719a260>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2da1719aad0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some visualization and shape printing below to make sure is loaded as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "val_samples = []\n",
    "val_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for sample, label in random.sample(list(train_data), k=3):\n",
    "  train_samples.append(sample)\n",
    "  train_labels.append(label)\n",
    "\n",
    "for sample, label in random.sample(list(val_data), k=3):\n",
    "  val_samples.append(sample)\n",
    "  val_labels.append(label)\n",
    "\n",
    "for sample, label in random.sample(list(test_data), k=3):\n",
    "  test_samples.append(sample)\n",
    "  test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: 3\n",
      "val shape: torch.Size([1, 28, 28])\n",
      "test shape: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'train length: {len(train_samples)}')\n",
    "print(f'val shape 0: {val_samples[0].shape}')\n",
    "print(f'test shape 2: {test_samples[2].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to do a little visualization just to make sure the data is how I expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Trouser')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.imshow(train_samples[0].squeeze(), cmap='gray')\n",
    "plt.title(class_names[train_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sneaker')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(val_samples[0].squeeze(), cmap='gray')\n",
    "plt.title(class_names[val_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'T-shirt/top')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_samples[0].squeeze(), cmap='gray')\n",
    "plt.title(class_names[test_labels[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.2: Generalized model\n",
    "\n",
    "* Create a `General` model function (or class) that takes hyper-parameters and evaluates the model\n",
    "  * The function should work with a set of hyper parameters than can be easily be controlled and varied by the user (for later parameter tuning)\n",
    "  * This should work for the training, test, and validation set \n",
    "* Feel free to recycle code from the lab assignments and demo's  \n",
    "* Use the deep learning best practices that we discussed in class. \n",
    "* Document what is going on in the code, as needed, with narrative markdown text between cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use a resnet like architecture for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasicResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block for a ResNet model with two convolutional layers, a skip connection, and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.25, padding=2):\n",
    "        super().__init__()\n",
    "        #conv layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        #conv layer 2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        #shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  \n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)  \n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class FashionMNISTResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple ResNet-like model for Fashion MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, channels, output_shape):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape, channels[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.blocks = nn.Sequential(*[BasicResBlock(channels[i], channels[i+1], stride=2 if i else 1)\n",
    "                                      for i in range(len(channels)-1)])\n",
    "        self.linear = nn.Linear(channels[-1], output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.blocks(out)\n",
    "        out = nn.AdaptiveAvgPool2d((1, 1))(out) #global average pooling\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.3: Model training function\n",
    "\n",
    "* You can do this in either a function (or python class), or however you think is best. \n",
    "* **Create a training function** (or class) that takes hyper-parameter choices and trains the model\n",
    "  * If you are doing \"leave one out\", your training function only needs to do one training per hyper-parameter choice\n",
    "  * If you are doing K-fold cross validation, you should train the model K times for each hyper-parameter choice, and report the average result cross the training runs at the end (this is technically a better practice but requires more computation). \n",
    "  * Use a dense feed forward ANN model, with the correct output layer activation, and correct loss function\n",
    "  * `You MUST use early stopping` inside the function, otherwise it defeats the point\n",
    "  * **Have at least the following hyper-parameters as inputs to this function** \n",
    "    * L1 regularization constant, L2 regularization constant, dropout rate \n",
    "    * Learning rate\n",
    "    * Weight Initialization: Fully random vs Xavier Weight Initialization\n",
    "    * Hidden layer activation function choice (use relu, sigmoid, or tanh)\n",
    "    * Number and size of ANN hidden layers \n",
    "    * Optimizer choice, have at least three included (Adam, SGD, or RmsProp)\n",
    "    * You can wrap all of the hyper-parameter arguments into a dictionary, or do it however you want  \n",
    "  * **Visualization**\n",
    "    * Include a boolean parameter as a function input that controls whether visualization is created or not\n",
    "    * If `true`, Monitor training and validation throughout training by plotting\n",
    "    * Report a confusion matrix \n",
    "  * Return the final training and validation error (averaged if using K-fold)\n",
    "    * again, you must use early stopping to report the best training/validation loss without over-fitting\n",
    "* Depending how you do this, it can be a lot of computation, start small and scale up and consider using Co-lab \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, early_stopping_patience=10, visualize=True):\n",
    "    \"\"\"\n",
    "    Trains a given model based on the provided parameters and data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to train.\n",
    "    - dataloaders: A dictionary containing 'train' and 'val' DataLoader objects.\n",
    "    - criterion: Loss function.\n",
    "    - optimizer: Optimization algorithm.\n",
    "    - scheduler: Learning rate scheduler.\n",
    "    - num_epochs: Number of epochs to train for.\n",
    "    - early_stopping_patience: Number of epochs to wait for improvement before early stopping.\n",
    "    - visualize: If True, plot training and validation losses.\n",
    "\n",
    "    Returns:\n",
    "    - model: The trained model.\n",
    "    - best_val_loss: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0  #early stopping counter\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()   \n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            #iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                #zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    #\n",
    "                    if phase == 'train':\n",
    "                        #backward  \n",
    "                        loss.backward()\n",
    "                        #step\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            #deep copy the model if it's the best one\n",
    "            if phase == 'val':\n",
    "                val_losses.append(epoch_loss)\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "            else:\n",
    "                train_losses.append(epoch_loss)\n",
    "\n",
    "        #implement early stopping if val loss doesn't improve \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping initiated\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "\n",
    "    if visualize:\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.plot(val_losses, label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.4: Hyper-parameter tuning\n",
    "\n",
    "* Keep detailed records of hyper-parameter choices and associated training & validation errors\n",
    "* Think critically and visualize the results of the search as needed\n",
    "\n",
    "* **Do each of these in a different sub-section of your notebook**\n",
    "  \n",
    "* **Explore hyper-parameter choice-0**\n",
    "  * for hidden activation=Relu, hidden layers = [32,32], optimizer=adam\n",
    "  * Vary the learning rate via a grid search pattern\n",
    "  * Plot training and validation error as a function of the learning rate\n",
    "  * Repeat this exercise for both random and Xavier weight initialization \n",
    "\n",
    "* **Explore hyper-parameter choice-1**\n",
    "  * for hidden activation=relu, hidden layers = [64,64], optimizer=adam\n",
    "  * Vary L1 and L2 in a 10x10 grid search (without dropout) \n",
    "  * Plot validation and training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "\n",
    "* **Explore hyper-parameter choice-2**\n",
    "  * for hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=**rmsprop**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation error as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* **Explore hyper-parameter choice-3:**\n",
    "  * for hidden activation=relu, hidden layers = [96,96,96], optimizer=**adam**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* `Optional` Systematically search for the best regularization parameters choice (3D search) using random search algorithm \n",
    "  * (https://en.wikipedia.org/wiki/Random_search)[https://en.wikipedia.org/wiki/Random_search]\n",
    "  * Try to see how deep you can get the ANN (max hidden layers) without suffering from the vanishing gradient effect  \n",
    "  \n",
    "* `Final fit`\n",
    "  * At the very end, select a best fit model and report, training, validation, and test errors at the very end\n",
    "  * Make sure your \"plotting variable=True\" when for the final training\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#initialize model\n",
    "def init_weights(m, init_type=\"random\"):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        if init_type == \"xavier\":\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        else:\n",
    "            nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "init_types = [\"random\", \"xavier\"]\n",
    "results = defaultdict(list)\n",
    "\n",
    "for init_type in init_types:\n",
    "    for lr in learning_rates:\n",
    "        model = FashionMNISTResNet(input_shape=1, channels=[32, 32, 64, 64], output_shape=10)\n",
    "        model.apply(lambda m: init_weights(m, init_type=init_type))\n",
    "\n",
    "        #loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "        #train model\n",
    "        trained_model, best_val_loss = train_model(model, {'train': train_dataloader, 'val': val_dataloader},\n",
    "                                                   criterion, optimizer, scheduler,\n",
    "                                                   num_epochs=10, early_stopping_patience=5, visualize=False)\n",
    "\n",
    "        #save results\n",
    "        results[init_type].append({'lr': lr, 'best_val_loss': best_val_loss})\n",
    "\n",
    "#visualize\n",
    "for init_type in init_types:\n",
    "    lrs = [result['lr'] for result in results[init_type]]\n",
    "    losses = [result['best_val_loss'] for result in results[init_type]]\n",
    "    plt.plot(lrs, losses, label=f\"{init_type} init\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Learning Rate vs. Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus assignment \n",
    "\n",
    "`+5 bonus points`\n",
    "\n",
    "`You DO NOT need to do this if you don't want to`\n",
    "\n",
    "* Once the data is collected, this HW should be quite easy, since most of the code can be recycled from the labs & textbook. \n",
    "\n",
    "* Do this in a file called `bonus.ipynb`, have it save its results to a folder \"data\"\n",
    "\n",
    "`Data collection`\n",
    "\n",
    "* Develope a text based classification data-set:\n",
    "* Use the Wikipedia API to search for articles to generate the data-set\n",
    "* Select a set of highly different topics (i.e. labels), for example,\n",
    "  * multi-class case: y=(pizza, oak_trees, basketball, ... , etc)=(0,1,2, ... , N-1)\n",
    "  * You don't have to use these, you can use whatever labels you want\n",
    "  * `Have AT LEAST 10 labels.` \n",
    "  * The more different the topics, the easier the classification task should be \n",
    "* Search for Wikipedia pages about these topics and harvest the text from the pages. \n",
    "* Do some basic text cleaning as needed. \n",
    "  * e.g. use the NLTK sentence tokenizer to break the text into sentences. \n",
    "  * Then form chunks of text that are five sentences long as your \"inputs\".\n",
    "* The \"label\" for these chunks will be the search label used to find the text. \n",
    "* The data set will not be perfect. \n",
    "  * There will be chunks of text that are not related to the topic (i.e. noise). \n",
    "  * However that is just something we have to live with.\n",
    "* **Important**: Always start small when writing & debugging THEN scale up. \n",
    "* The more chunks of text you have the better.\n",
    "  * Save the text and labels to the same format used by the textbook, that way you can recycle your lab code seamlessly. \n",
    "* `Optional practice`: You can also \"tag\" each chunk of text with an associated \"compound\" sentiment score computed using the NLTK sentiment analysis. From this you can train a regression model in part-2. This is somewhat silly, and is just for educational purposes, since your using a model output to train another model. \n",
    "\n",
    "`Model training`\n",
    "\n",
    "* Repeat the model training and hyper-parameter tuning exercise for MNIST, but with your text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bbb781ca6673b7d7a2eaec0820775eebfaab6ec1fac7365fb415515f8c23aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
