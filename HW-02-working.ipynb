{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "format: \n",
    "    html: \n",
    "        embed-resources: true\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: \n",
    "\n",
    "`ANN training in Keras or Pytorch & Hyper-parameter tuning`\n",
    "\n",
    "## Overview \n",
    "\n",
    "* Classification is one of the most common forms of supervised machine learning \n",
    "* In this homework we will explore \"model tuning\" for the case of a multi-class classification problem, as applied the MNIST data set\n",
    "* `You can do this assignment in either Keras OR PyTorch` (or both), it is your choice.\n",
    "\n",
    "## Submission \n",
    "\n",
    "* You need to upload TWO documents to Canvas when you are done\n",
    "  * (1) A PDF (or HTML) of the completed form of the `HW-2.ipynb` document \n",
    "* The final uploaded version should NOT have any code-errors present \n",
    "* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\n",
    "\n",
    "`IMPORTANT`: THERE ARE MANY WAYS TO DO THIS, SO FEEL FREE TO DEVIATE SLIGHTLY FROM THE EXACT DETAILS, BUT THE OVERALL RESULT AND FLOW SHOULD MATCH WHAT IS OUTLINED BELOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.1: Data preparation\n",
    "\n",
    "* Normalize the data as needed\n",
    "* Partition data into training, validation, and test (i.e. leave one out CV)\n",
    "  * One option to do this is to give these arrays global scope so they are seen inside the training function (so they don't need to be passed to functions)\n",
    "* **Optional but recommended:** Create a K-fold cross validation data set, rather than just doing leave one out\n",
    "* Do any other preprocessing you feel is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Fashion MNIST\n",
    "\n",
    "99.999% sure Dr. Hickman said it was ok to use Fashion MNIST instead of regular MNIST, which I will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424486c8c8554e559c54ff9a4f512aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77e79fc66da4ffe91ace7d7b5461d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa3f86387c848a39997dcb1bec0107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e2d2ed73f74e9cb6d00e637a0a9091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2d9ee7b87c0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2d9daa2ac20>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.2: Generalized model\n",
    "\n",
    "* Create a `General` model function (or class) that takes hyper-parameters and evaluates the model\n",
    "  * The function should work with a set of hyper parameters than can be easily be controlled and varied by the user (for later parameter tuning)\n",
    "  * This should work for the training, test, and validation set \n",
    "* Feel free to recycle code from the lab assignments and demo's  \n",
    "* Use the deep learning best practices that we discussed in class. \n",
    "* Document what is going on in the code, as needed, with narrative markdown text between cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.3: Model training function\n",
    "\n",
    "* You can do this in either a function (or python class), or however you think is best. \n",
    "* **Create a training function** (or class) that takes hyper-parameter choices and trains the model\n",
    "  * If you are doing \"leave one out\", your training function only needs to do one training per hyper-parameter choice\n",
    "  * If you are doing K-fold cross validation, you should train the model K times for each hyper-parameter choice, and report the average result cross the training runs at the end (this is technically a better practice but requires more computation). \n",
    "  * Use a dense feed forward ANN model, with the correct output layer activation, and correct loss function\n",
    "  * `You MUST use early stopping` inside the function, otherwise it defeats the point\n",
    "  * **Have at least the following hyper-parameters as inputs to this function** \n",
    "    * L1 regularization constant, L2 regularization constant, dropout rate \n",
    "    * Learning rate\n",
    "    * Weight Initialization: Fully random vs Xavier Weight Initialization\n",
    "    * Hidden layer activation function choice (use relu, sigmoid, or tanh)\n",
    "    * Number and size of ANN hidden layers \n",
    "    * Optimizer choice, have at least three included (Adam, SGD, or RmsProp)\n",
    "    * You can wrap all of the hyper-parameter arguments into a dictionary, or do it however you want  \n",
    "  * **Visualization**\n",
    "    * Include a boolean parameter as a function input that controls whether visualization is created or not\n",
    "    * If `true`, Monitor training and validation throughout training by plotting\n",
    "    * Report a confusion matrix \n",
    "  * Return the final training and validation error (averaged if using K-fold)\n",
    "    * again, you must use early stopping to report the best training/validation loss without over-fitting\n",
    "* Depending how you do this, it can be a lot of computation, start small and scale up and consider using Co-lab \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.4: Hyper-parameter tuning\n",
    "\n",
    "* Keep detailed records of hyper-parameter choices and associated training & validation errors\n",
    "* Think critically and visualize the results of the search as needed\n",
    "\n",
    "* **Do each of these in a different sub-section of your notebook**\n",
    "  \n",
    "* **Explore hyper-parameter choice-0**\n",
    "  * for hidden activation=Relu, hidden layers = [32,32], optimizer=adam\n",
    "  * Vary the learning rate via a grid search pattern\n",
    "  * Plot training and validation error as a function of the learning rate\n",
    "  * Repeat this exercise for both random and Xavier weight initialization \n",
    "\n",
    "* **Explore hyper-parameter choice-1**\n",
    "  * for hidden activation=relu, hidden layers = [64,64], optimizer=adam\n",
    "  * Vary L1 and L2 in a 10x10 grid search (without dropout) \n",
    "  * Plot validation and training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of L1 and L2 regularization in a 2D heatmap \n",
    "\n",
    "* **Explore hyper-parameter choice-2**\n",
    "  * for hidden activation=sigmoid, hidden layers = [96,96,96], optimizer=**rmsprop**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation error as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* **Explore hyper-parameter choice-3:**\n",
    "  * for hidden activation=relu, hidden layers = [96,96,96], optimizer=**adam**\n",
    "  * Vary drop-out parameter in a 1x10 grid search (without L1 or L2 regularization) \n",
    "  * Plot training and validation as a function of dropout rate  \n",
    "  * Plot the ratio (or difference) of validation to training error as a function of dropout rate  \n",
    "\n",
    "* `Optional` Systematically search for the best regularization parameters choice (3D search) using random search algorithm \n",
    "  * (https://en.wikipedia.org/wiki/Random_search)[https://en.wikipedia.org/wiki/Random_search]\n",
    "  * Try to see how deep you can get the ANN (max hidden layers) without suffering from the vanishing gradient effect  \n",
    "  \n",
    "* `Final fit`\n",
    "  * At the very end, select a best fit model and report, training, validation, and test errors at the very end\n",
    "  * Make sure your \"plotting variable=True\" when for the final training\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus assignment \n",
    "\n",
    "`+5 bonus points`\n",
    "\n",
    "`You DO NOT need to do this if you don't want to`\n",
    "\n",
    "* Once the data is collected, this HW should be quite easy, since most of the code can be recycled from the labs & textbook. \n",
    "\n",
    "* Do this in a file called `bonus.ipynb`, have it save its results to a folder \"data\"\n",
    "\n",
    "`Data collection`\n",
    "\n",
    "* Develope a text based classification data-set:\n",
    "* Use the Wikipedia API to search for articles to generate the data-set\n",
    "* Select a set of highly different topics (i.e. labels), for example,\n",
    "  * multi-class case: y=(pizza, oak_trees, basketball, ... , etc)=(0,1,2, ... , N-1)\n",
    "  * You don't have to use these, you can use whatever labels you want\n",
    "  * `Have AT LEAST 10 labels.` \n",
    "  * The more different the topics, the easier the classification task should be \n",
    "* Search for Wikipedia pages about these topics and harvest the text from the pages. \n",
    "* Do some basic text cleaning as needed. \n",
    "  * e.g. use the NLTK sentence tokenizer to break the text into sentences. \n",
    "  * Then form chunks of text that are five sentences long as your \"inputs\".\n",
    "* The \"label\" for these chunks will be the search label used to find the text. \n",
    "* The data set will not be perfect. \n",
    "  * There will be chunks of text that are not related to the topic (i.e. noise). \n",
    "  * However that is just something we have to live with.\n",
    "* **Important**: Always start small when writing & debugging THEN scale up. \n",
    "* The more chunks of text you have the better.\n",
    "  * Save the text and labels to the same format used by the textbook, that way you can recycle your lab code seamlessly. \n",
    "* `Optional practice`: You can also \"tag\" each chunk of text with an associated \"compound\" sentiment score computed using the NLTK sentiment analysis. From this you can train a regression model in part-2. This is somewhat silly, and is just for educational purposes, since your using a model output to train another model. \n",
    "\n",
    "`Model training`\n",
    "\n",
    "* Repeat the model training and hyper-parameter tuning exercise for MNIST, but with your text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bbb781ca6673b7d7a2eaec0820775eebfaab6ec1fac7365fb415515f8c23aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
